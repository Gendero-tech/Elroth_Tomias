import requests
import os
import random
import time

def call_ollama(model, prompt, images=None):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "keep_alive": 0 # Force le mod√®le √† s'effacer apr√®s la r√©ponse
    }
    if images:
        payload["images"] = images
    
    try:
        r = requests.post(url, json=payload, timeout=60)
        return r.json().get("response", "")
    except:
        return ""

def run():
    path = "training_data/BackpackBattles"
    images = [f for f in os.listdir(path) if f.endswith('.jpg')]
    if not images: return
    
    target = os.path.join(path, random.choice(images))
    import base64
    with open(target, "rb") as f:
        img_data = base64.b64encode(f.read()).decode('utf-8')

    print("üì∏ Clio regarde l'image...")
    # On demande √† Moondream une description tr√®s courte pour √©conomiser la RAM
    desc = call_ollama("moondream", "What items are in the backpack? (Short list)", [img_data])
    
    if not desc:
        print("‚ùå Moondream n'a pas r√©pondu.")
        return

    print(f"üëÄ Description : {desc[:50]}...")
    print("‚è≥ Pause technique (Lib√©ration de la RAM)...")
    time.sleep(3) # On laisse 3 secondes pour que le GPU respire

    print("üß† Phi-3 r√©fl√©chit...")
    final = call_ollama("phi3", f"Tu es l'IA de Ambre. Commente bri√®vement ce build : {desc}")
    
    print("\n--- ü§ñ CLIO ---")
    if final:
        print(final)
    else:
        # Si Phi-3 √©choue, on donne une r√©ponse de secours "IA"
        print(f"Maman, je vois des objets ({desc[:30]}), mais mon cerveau est trop plein pour analyser !")
    print("----------------\n")

if __name__ == "__main__":
    run()