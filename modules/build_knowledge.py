# Fichier : modules/build_knowledge.py
# (√Ä lancer via 'python -m modules.build_knowledge' pour entra√Æner Clio)
# Correction: Collecte toutes les donn√©es en m√©moire avant l'ajout unique pour FAISS (IndexIVFPQ).

import trafilatura
from langchain_text_splitters import RecursiveCharacterTextSplitter
# Imports relatifs
from .clio_knowledge import ClioKnowledge
from .clio_vector_memory import ClioVectorMemory
from .youtube import YoutubeClient
import time
import re 
import os 

# --- CONFIGURATION ---

print("Initialisation de la base de connaissance (RAG)...")
# üö® NOTE: Les chemins doivent √™tre pass√©s par le constructeur de ClioVectorMemory dans Memory.py, 
# mais ici, nous utilisons les chemins par d√©faut pour le script autonome.
kb = ClioVectorMemory() 

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200, 
    separators=["\n\n", "\n", ". ", " ", ""]
)

sources_dict = ClioKnowledge().sources
youtube_client = YoutubeClient(None) 

# Liste des URLs d√©j√† pr√©sentes (collect√©es √† partir de la m√©moire vectorielle)
processed_urls = set()
for meta in kb.metadata:
    if 'source' in meta:
        processed_urls.add(meta['source'])
print(f"{len(processed_urls)} URLs d√©j√† pr√©sentes dans la base.")

# --- FONCTIONS UTILES ---

def is_youtube_url(url: str) -> bool:
    """V√©rifie si l'URL est un lien YouTube."""
    return bool(re.search(r'(youtube\.com/watch\?v=|youtu\.be/)', url))

# --- EX√âCUTION ---

def build_knowledge_base():
    if kb.model is None:
        print("\n‚ùå ERREUR: Le mod√®le d'encodage vectoriel (SentenceTransformer) n'a pas pu √™tre charg√©. L'entra√Ænement est impossible.")
        return
        
    # --- PHASE 1: COLLECTE GLOBALE DES SEGMENTS ---
    print("\n--- PHASE 1: COLLECTE DES DONN√âES DE TOUTES LES SOURCES ---")
    all_chunks = []
    all_metadatas = []
    
    # 1. Parcourir tous les domaines et collecter les donn√©es
    for domain, urls in sources_dict.items():
        print(f"\n[Collecte] Traitement du domaine : {domain}")
        
        for url in urls:
            if url in processed_urls:
                print(f"Saut (d√©j√† index√©) : {url}")
                continue

            print(f"Collecte de : {url} ...")
            
            try:
                main_content = None
                
                if is_youtube_url(url):
                    # Utilisation du client YouTube
                    transcript_text = youtube_client.api.get_transcript(url)
                    main_content = transcript_text
                else:
                    # Utilisation de Trafilatura (Articles Web)
                    downloaded = trafilatura.fetch_url(url)
                    if downloaded:
                        main_content = trafilatura.extract(downloaded)

                # üöÄ AM√âLIORATION : V√©rification de la validit√© du contenu
                if not main_content or main_content.strip() == "" or len(main_content) < 50:
                    print(f"√âchec de l'extraction (Contenu vide ou trop court) : {url}")
                    # Marque l'URL comme trait√©e (pour √©viter de la retenter si elle est vide)
                    processed_urls.add(url) 
                    continue

                # D√©coupage et ajout temporaire
                chunks = text_splitter.split_text(main_content)
                
                if not chunks:
                     print(f"ALERTE: Extraction r√©ussie mais aucun segment g√©n√©r√© pour {url}")
                     processed_urls.add(url)
                     continue
                     
                for chunk in chunks:
                    all_chunks.append(chunk)
                    # üö® AM√âLIORATION : Assure que la source est dans la m√©tadonn√©e du chunk
                    all_metadatas.append({
                        "text": chunk, "source": url, "domain": domain
                    })
                
                # Marque l'URL comme trait√©e pour les futures ex√©cutions
                processed_urls.add(url)

                print(f"Collect√© {len(chunks)} segments (Total: {len(all_chunks)})")
                time.sleep(1) 

            except Exception as e:
                print(f"ERREUR PENDANT LA COLLECTE sur {url}: {e}")

    print(f"\n--- PHASE 1 TERMIN√âE. {len(all_chunks)} segments collect√©s au total. ---")
    
    # V√©rification de la quantit√© de donn√©es avant entra√Ænement
    if len(all_chunks) < kb.index.nlist: 
        print(f"ALERTE RAG: Seulement {len(all_chunks)} segments collect√©s. L'Index FAISS requiert au moins {kb.index.nlist} (id√©alement 400+). L'entra√Ænement pourrait √©chouer.")
        
    if not all_chunks:
        print("Aucune nouvelle donn√©e √† ajouter. Termin√©.")
        return

    # --- PHASE 2: ENTRA√éNEMENT ET AJOUT EN UN SEUL LOT ---
    print("\n--- PHASE 2: ENTRA√éNEMENT ET AJOUT DU LOT GLOBAL ---")
    
    try:
        # Ajout du lot global, ce qui d√©clenchera l'entra√Ænement FAISS une seule fois
        kb.batch_add_segments(all_chunks, all_metadatas)
        
        # üö® CORRECTION CRITIQUE : La sauvegarde finale des m√©tadonn√©es n'est plus n√©cessaire ici.
        # ClioVectorMemory.batch_add_segments sauvegarde l'index et les m√©tadonn√©es.
        # Si la m√©thode ClioVectorMemory.batch_add_segments est correctement impl√©ment√©e, elle a d√©j√† sauvegard√© l'index.
        # Cependant, pour s'assurer que les URLs qui n'ont pas produit de segments (mais qui ont √©t√© v√©rifi√©es)
        # ne soient pas retent√©es, on peut ajouter une logique de nettoyage. (Ignor√© ici car toutes les URLs v√©rifi√©es ont produit des segments)
        
        print("\n‚úÖ‚úÖ‚úÖ Mise √† jour de la base de connaissance (RAG) termin√©e ! Clio est maintenant entra√Æn√©e. ‚úÖ‚úÖ‚úÖ")

    except Exception as e:
        print(f"\n‚ùå ERREUR CRITIQUE PENDANT LA PHASE 2 (ENTRA√éNEMENT/AJOUT) : {e}")
        print("L'entra√Ænement a √©chou√©. V√©rifiez vos d√©pendances FAISS ou la quantit√© de donn√©es.")


if __name__ == "__main__":
    build_knowledge_base()